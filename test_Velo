#!/bin/bash
#SBATCH -o hostname_%j.out
# File to which STDOUT will be written, including job ID
#SBATCH -e hostname_%j.err
# File to which STDERR will be written, including job ID

################## Step0: make loom file  #############

module load gcc/6.2.0
module load conda2/4.2.13
module load python/3.6.0
module load samtools/1.9
source activate yuj626

repeats="/n/scratch3/users/y/yuj626/mm10/mm10_rmsk.gtf" 
transcriptome="/n/scratch3/users/y/yuj626/mm10/ref.genes.gtf" 

velocyto run10x -m /n/scratch3/users/y/yuj626/mm10/mm10_rmsk.gtf -@ 4 --samtools-memory 7000 /n/scratch3/users/y/yuj626/peri_ct3 /n/scratch3/users/y/yuj626/mm10/ref.genes.gtf
velocyto run10x -m /n/scratch3/users/y/yuj626/mm10/mm10_rmsk.gtf -@ 4 --samtools-memory 7000 /n/scratch3/users/y/yuj626/peri_ko1 /n/scratch3/users/y/yuj626/mm10/ref.genes.gtf

conda deactivate 

################## Step0: load sample1 #############

import scanpy as sc
import anndata
from scipy import io
from scipy.sparse import coo_matrix, csr_matrix
import numpy as np
import os
import pandas as pd

# load sparse matrix:
X = io.mmread("/n/scratch3/users/y/yuj626/velo/yizhong/peri_ct3/scVelo_counts.mtx")

# create anndata object
adata = anndata.AnnData(X=X.transpose().tocsr())

# load cell metadata:
cell_meta = pd.read_csv("/n/scratch3/users/y/yuj626/velo/yizhong/peri_ct3/scVelo_metadata.csv")

# load gene names:
with open("/n/scratch3/users/y/yuj626/velo/yizhong/peri_ct3/scVelo_gene_names.csv", 'r') as f:
    gene_names = f.read().splitlines()

# set anndata observations and index obs by barcodes, var by gene names
adata.obs = cell_meta
adata.obs.index = adata.obs['barcode']
adata.var.index = gene_names

# load dimensional reduction:
pca = pd.read_csv("/n/scratch3/users/y/yuj626/velo/yizhong/peri_ct3/scVelo_pca.csv")
pca.index = adata.obs.index

# set pca and umap
adata.obsm['X_pca'] = pca.to_numpy()
adata.obsm['X_umap'] = np.vstack((adata.obs['UMAP_1'].to_numpy(), adata.obs['UMAP_2'].to_numpy())).T

# plot a UMAP colored by sampleID to test:
sc.pl.umap(adata, color=['seurat_clusters'], frameon=False, save=True)

# save dataset as anndata format
adata.write('peri_ct3.h5ad')

# reload dataset
adata = sc.read_h5ad('peri_ct3.h5ad')

############# Step 1: Load data ###########

import scvelo as scv
import scanpy as sc
import cellrank as cr
import numpy as np
import pandas as pd
import anndata as ad

scv.settings.verbosity = 3
scv.settings.set_figure_params('scvelo', facecolor='white', dpi=100, frameon=False)
cr.settings.verbosity = 2

adata = sc.read_h5ad('peri_ct3.h5ad')
ldata = scv.read('peri_ct3.loom', cache=True)

# rename barcodes in order to merge:
barcodes = [bc.split(':')[1] for bc in ldata.obs.index.tolist()]
barcodes = [bc[0:len(bc)-1] + '_10' for bc in barcodes]
ldata.obs.index = barcodes
ldata.var_names_make_unique()
scv.utils.clean_obs_names(adata)
scv.utils.clean_obs_names(ldata)
common_obs = adata.obs_names.intersection(ldata.obs_names)
common_vars = adata.var_names.intersection(ldata.var_names)
print(len(common_obs), len(common_vars))
adata = scv.utils.merge(adata, ldata)

# plot umap to check
sc.pl.umap(adata, color='seurat_clusters', frameon=False, legend_loc='on data', title='', save='umap.pdf')

############## Step 2: Computing RNA velocity using scVelo ################

scv.pl.proportions(adata, groupby='seurat_clusters',save='proportions.pdf')

# pre-process
scv.pp.filter_and_normalize(adata)
scv.pp.moments(adata)

# compute velocity
scv.tl.velocity(adata, mode='stochastic')
scv.tl.velocity_graph(adata)

#Visualize velocity fields
scv.pl.velocity_embedding(adata, basis='umap', frameon=False, save='embedding.pdf')
scv.pl.velocity_embedding_grid(adata, basis='umap', color='seurat_clusters', save='embedding_grid.pdf', title='', scale=0.25)
scv.pl.velocity_embedding_stream(adata, basis='umap', color=['seurat_clusters'], save='embedding_stream.pdf', title='')
scv.tl.velocity_pseudotime(adata)
scv.pl.scatter(adata, color='velocity_pseudotime', cmap='gnuplot', save='velocity_pseudotime.pdf')
